{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Calculating word error rate for the Wav2Vec2 Base model on the test dataset","metadata":{}},{"cell_type":"code","source":"#Installing dependencies\n%%capture\n!pip install git+https://github.com/huggingface/datasets.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install torchaudio\n!pip install librosa\n!pip install jiwer\n!pip install wandb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import Dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/test-data/test.tsv\", sep=\"\\t\")\n# df[\"path\"] = \"/content/drive/clips/\" + df[\"path\"]\ntest_dataset = Dataset.from_pandas(df)\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"akanksha-b14/songs_transcription_wav2vec_base2\",use_auth_token=\"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\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"akanksha-b14/songs_transcription_wav2vec_base2\",use_auth_token=\"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\") \nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\_\\;\\:\\\"\\“\\%\\‘\\”\\।\\’\\'\\&]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\ndef normalizer(text):\n    # Use your custom normalizer\n    text = text.replace(\"\\\\n\",\"\\n\")\n    text = ' '.join(text.split())\n    text = re.sub(r'''(/|-|_)''',\" \", text)\n    text = text.strip()\n    return text\n\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = normalizer(batch[\"sentence\"])\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()+ \" \"\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating word error rate for the Wav2Vec2 XLSR model on the test dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import Dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/test-data/test.tsv\", sep=\"\\t\")\n# df[\"path\"] = \"/content/drive/clips/\" + df[\"path\"]\ntest_dataset = Dataset.from_pandas(df)\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"akanksha-b14/songs-transcription\",use_auth_token=\"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\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"akanksha-b14/songs-transcription\",use_auth_token=\"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\") \nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\_\\;\\:\\\"\\“\\%\\‘\\”\\।\\’\\'\\&]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\ndef normalizer(text):\n    # Use your custom normalizer\n    text = text.replace(\"\\\\n\",\"\\n\")\n    text = ' '.join(text.split())\n    text = re.sub(r'''(/|-|_)''',\" \", text)\n    text = text.strip()\n    return text\n\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = normalizer(batch[\"sentence\"])\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()+ \" \"\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:12:07.827545Z","iopub.execute_input":"2022-04-10T13:12:07.828387Z","iopub.status.idle":"2022-04-10T13:13:29.255002Z","shell.execute_reply.started":"2022-04-10T13:12:07.828313Z","shell.execute_reply":"2022-04-10T13:13:29.25412Z"},"trusted":true},"execution_count":null,"outputs":[]}]}